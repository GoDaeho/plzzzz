{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDPHft_VSS5H"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def conv1d_block(inp, cweight, bweight, activation=tf.nn.relu):\n",
        "    \"\"\"Custom 1D convolutional block for MAML.\"\"\"\n",
        "    conv_output = tf.nn.conv1d(input=inp, filters=cweight, stride=1, padding='SAME') + bweight\n",
        "    return activation(conv_output)\n",
        "\n",
        "class CustomConv1D(tf.keras.layers.Layer):\n",
        "    def __init__(self, filter_size, num_filters):\n",
        "        super(CustomConv1D, self).__init__()\n",
        "        self.num_filters = num_filters\n",
        "        self.filter_size = filter_size\n",
        "\n",
        "        weight_initializer = tf.keras.initializers.GlorotUniform()\n",
        "        self.cweight = tf.Variable(weight_initializer(shape=[self.filter_size, 1, self.num_filters]), name='cweight')\n",
        "        self.bweight = tf.Variable(tf.zeros([self.num_filters]), name='bweight')\n",
        "\n",
        "    def call(self, inp):\n",
        "        return conv1d_block(inp, self.cweight, self.bweight)\n",
        "\n",
        "class MAMLCNNClassifier(tf.keras.Model):\n",
        "    def __init__(self, **kargs):\n",
        "        super(MAMLCNNClassifier, self).__init__(name=kargs['model_name'])\n",
        "        self.embedding = layers.Embedding(input_dim=kargs['vocab_size'], output_dim=kargs['embedding_size'])\n",
        "\n",
        "        # Custom Conv1D layers for MAML\n",
        "        self.conv_list = [CustomConv1D(kernel_size, kargs['num_filters']) for kernel_size in [3, 4, 5]]\n",
        "\n",
        "        self.pooling = layers.GlobalMaxPooling1D()\n",
        "        self.dropout = layers.Dropout(kargs['dropout_rate'])\n",
        "        self.fc = layers.Dense(units=kargs['output_dimension'], activation='sigmoid')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x)\n",
        "        # Ensure x is correctly reshaped if necessary before this step\n",
        "        x = tf.concat([self.pooling(conv(x)) for conv in self.conv_list], axis=1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "class TextDataGenerator:\n",
        "    def __init__(self, texts, labels, categories, num_samples_per_class, max_len=100, num_words=10000):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.categories = categories  # A list of unique categories\n",
        "        self.num_samples_per_class = num_samples_per_class\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = Tokenizer(num_words=num_words)\n",
        "        self._create_tokenizer()\n",
        "        self._create_category_map()\n",
        "\n",
        "    def _create_tokenizer(self):\n",
        "        self.tokenizer.fit_on_texts(self.texts)\n",
        "\n",
        "    def _create_category_map(self):\n",
        "        self.category_map = {}\n",
        "        for category in self.categories:\n",
        "            self.category_map[category] = [(text, label) for text, label, cat in zip(self.texts, self.labels, self.categories) if cat == category]\n",
        "\n",
        "    def sample_batch(self, batch_size, shuffle=True):\n",
        "        batch_texts, batch_labels = [], []\n",
        "        for _ in range(batch_size):\n",
        "            sampled_categories = random.sample(self.categories, self.num_samples_per_class)\n",
        "            for category in sampled_categories:\n",
        "                samples = random.sample(self.category_map[category], 1)\n",
        "                for text, label in samples:\n",
        "                    batch_texts.append(text)\n",
        "                    batch_labels.append(label)\n",
        "\n",
        "        # Tokenize and pad text sequences\n",
        "        sequences = self.tokenizer.texts_to_sequences(batch_texts)\n",
        "        padded_sequences = pad_sequences(sequences, maxlen=self.max_len, padding='post')\n",
        "\n",
        "        if shuffle:\n",
        "            combined = list(zip(padded_sequences, batch_labels))\n",
        "            random.shuffle(combined)\n",
        "            padded_sequences, batch_labels = zip(*combined)\n",
        "\n",
        "        return np.array(padded_sequences), np.array(batch_labels)\n",
        "\n"
      ],
      "metadata": {
        "id": "9VrCdXG3SXqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "\n",
        "class MAML(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, num_filters, output_dimension, dropout_rate, num_inner_updates=5, inner_update_lr=0.4, learn_inner_update_lr=False):\n",
        "        super(MAML, self).__init__()\n",
        "        self.inner_update_lr = inner_update_lr\n",
        "        self.loss_func = tf.keras.losses.BinaryCrossentropy()  # or CategoricalCrossentropy for multi-class\n",
        "        self.learn_inner_update_lr = learn_inner_update_lr\n",
        "\n",
        "        # Custom CNN for text data\n",
        "        self.cnn_model = MAMLCNNClassifier(model_name='maml_cnn', vocab_size=vocab_size, embedding_size=embedding_size, num_filters=num_filters, output_dimension=output_dimension, dropout_rate=dropout_rate)\n",
        "\n",
        "        if self.learn_inner_update_lr:\n",
        "            self.inner_update_lr_dict = {}\n",
        "            for layer in self.cnn_model.layers:\n",
        "                self.inner_update_lr_dict[layer.name] = [tf.Variable(self.inner_update_lr, name=f'inner_update_lr_{layer.name}_{j}') for j in range(num_inner_updates)]\n",
        "\n",
        "    def call(self, inp, meta_batch_size=25, num_inner_updates=1):\n",
        "        def task_inner_loop(inp, reuse=True, meta_batch_size=25, num_inner_updates=1):\n",
        "            input_tr, input_ts, label_tr, label_ts = inp\n",
        "\n",
        "            # weights corresponds to the initial weights in MAML (i.e. the meta-parameters)\n",
        "            weights = {layer.name: layer.trainable_weights for layer in self.cnn_model.layers}\n",
        "\n",
        "            task_outputs_ts, task_losses_ts, task_accuracies_ts = [], [], []\n",
        "\n",
        "            for i in range(num_inner_updates):\n",
        "                with tf.GradientTape(persistent=True) as inner_tape:\n",
        "                    task_output_tr_pre = self.cnn_model(input_tr)\n",
        "                    task_loss_tr_pre = self.loss_func(label_tr, task_output_tr_pre)\n",
        "\n",
        "                grads = inner_tape.gradient(task_loss_tr_pre, self.cnn_model.trainable_weights)\n",
        "                if self.learn_inner_update_lr:\n",
        "                    updated_weights = [weight - self.inner_update_lr_dict[weight.name][i] * grad for weight, grad in zip(self.cnn_model.trainable_weights, grads)]\n",
        "                else:\n",
        "                    updated_weights = [weight - self.inner_update_lr * grad for weight, grad in zip(self.cnn_model.trainable_weights, grads)]\n",
        "\n",
        "                # Manual weight update\n",
        "                for layer, new_weights in zip(self.cnn_model.layers, updated_weights):\n",
        "                    layer.set_weights(new_weights)\n",
        "\n",
        "                # Testing on the test set\n",
        "                task_output_ts = self.cnn_model(input_ts)\n",
        "                task_loss_ts = self.loss_func(label_ts, task_output_ts)\n",
        "                task_outputs_ts.append(task_output_ts)\n",
        "                task_losses_ts.append(task_loss_ts)\n",
        "\n",
        "            # Compute accuracies\n",
        "            task_accuracy_tr_pre = tf.keras.metrics.binary_accuracy(label_tr, tf.nn.softmax(task_output_tr_pre))  # or categorical_accuracy\n",
        "            task_accuracies_ts = [tf.keras.metrics.binary_accuracy(label_ts, tf.nn.softmax(output)) for output in task_outputs_ts]\n",
        "\n",
        "            task_output = [task_output_tr_pre, task_outputs_ts, task_loss_tr_pre, task_losses_ts, task_accuracy_tr_pre, task_accuracies_ts]\n",
        "\n",
        "            return task_output\n",
        "\n",
        "        input_tr, input_ts, label_tr, label_ts = inp\n",
        "\n",
        "        # Parallel processing for each task in the meta-batch\n",
        "        out_dtype = [tf.float32, [tf.float32] * num_inner_updates, tf.float32, [tf.float32] * num_inner_updates, tf.float32, [tf.float32] * num_inner_updates]\n",
        "        task_inner_loop_partial = partial(task_inner_loop, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
        "\n",
        "        result = tf.map_fn(task_inner_loop_partial, elems=(input_tr, input_ts, label_tr, label_ts), dtype=out_dtype, parallel_iterations=meta_batch_size)\n",
        "\n",
        "        return result"
      ],
      "metadata": {
        "id": "8RPPJrJqSXtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def outer_train_step(inp, model, optimizer, meta_batch_size=25, num_inner_updates=1):\n",
        "    with tf.GradientTape(persistent=False) as outer_tape:\n",
        "        result = model(inp, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
        "\n",
        "        # Unpack the output\n",
        "        _, _, losses_tr_pre, losses_ts, _, accuracies_ts = result\n",
        "        total_losses_ts = [tf.reduce_mean(loss_ts) for loss_ts in losses_ts]\n",
        "\n",
        "    gradients = outer_tape.gradient(total_losses_ts[-1], model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    # Calculate average pre-update loss and post-update accuracy\n",
        "    total_loss_tr_pre = tf.reduce_mean(losses_tr_pre)\n",
        "    total_accuracy_ts = [tf.reduce_mean(accuracy_ts) for accuracy_ts in accuracies_ts]\n",
        "\n",
        "    return total_loss_tr_pre, total_losses_ts, total_accuracy_ts\n",
        "\n",
        "def outer_eval_step(inp, model, meta_batch_size=25, num_inner_updates=1):\n",
        "    result = model(inp, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
        "\n",
        "    # Unpack the output\n",
        "    _, _, losses_tr_pre, losses_ts, _, accuracies_ts = result\n",
        "\n",
        "    total_loss_tr_pre = tf.reduce_mean(losses_tr_pre)\n",
        "    total_losses_ts = [tf.reduce_mean(loss_ts) for loss_ts in losses_ts]\n",
        "    total_accuracy_ts = [tf.reduce_mean(accuracy_ts) for accuracy_ts in accuracies_ts]\n",
        "\n",
        "    return total_loss_tr_pre, total_losses_ts, total_accuracy_ts\n",
        "\n",
        "def meta_train_fn(model, exp_string, data_generator, n_way, meta_train_iterations, meta_batch_size, log, logdir, k_shot, num_inner_updates, meta_lr):\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=meta_lr)\n",
        "\n",
        "    for itr in range(meta_train_iterations):\n",
        "        # Sample a batch of training data\n",
        "        batch_texts, batch_labels = data_generator.sample_batch(batch_size=meta_batch_size, shuffle=True)\n",
        "\n",
        "        # Prepare the input for the model\n",
        "        inp = (batch_texts, batch_labels)  # Adjust as per your data format\n",
        "\n",
        "        total_loss_tr_pre, total_losses_ts, total_accuracy_ts = outer_train_step(inp, model, optimizer, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
        "\n",
        "        if itr % 10 == 0:  # Print every 10 iterations\n",
        "            print(f\"Iteration {itr}: Pre-update loss: {total_loss_tr_pre}, Post-update accuracy: {total_accuracy_ts[-1]}\")\n",
        "\n",
        "        if itr % 100 == 0:  # Save model every 100 iterations\n",
        "            model_save_path = f\"{logdir}/{exp_string}/model_{itr}\"\n",
        "            model.save_weights(model_save_path)\n",
        "            print(f\"Saved model to {model_save_path}\")\n",
        "\n",
        "def meta_test_fn(model, data_generator, n_way, meta_batch_size, k_shot, num_inner_updates):\n",
        "    # Meta-testing function\n",
        "    num_test_tasks = 600  # Number of tasks to test on\n",
        "\n",
        "    meta_test_accuracies = []\n",
        "\n",
        "    for _ in range(num_test_tasks):\n",
        "        # Sample a batch of test data\n",
        "        batch_texts, batch_labels = data_generator.sample_batch(batch_size=meta_batch_size, shuffle=True)\n",
        "\n",
        "        # Prepare the input for the model\n",
        "        inp = (batch_texts, batch_labels)  # Adjust as per your data format\n",
        "\n",
        "        _, _, _, _, _, accuracies_ts = outer_eval_step(inp, model, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
        "        meta_test_accuracies.append(accuracies_ts[-1])\n",
        "\n",
        "    # Compute and print the average accuracy across all tasks\n",
        "    mean_accuracy = np.mean(meta_test_accuracies)\n",
        "    print(f\"Meta-Test Accuracy: {mean_accuracy}\")\n",
        "\n",
        "\n",
        "def run_maml(data_generator, n_way, k_shot, meta_batch_size, meta_lr,\n",
        "             inner_update_lr, num_filters, num_inner_updates,\n",
        "             learn_inner_update_lr, meta_train_iterations, logdir):\n",
        "    # Instantiate the MAML model\n",
        "    data_generator = TextDataGenerator()\n",
        "    model = MAML(vocab_size=data_generator.vocab_size, embedding_size=128, num_filters=num_filters, output_dimension=n_way, dropout_rate=0.5, num_inner_updates=num_inner_updates, inner_update_lr=inner_update_lr, learn_inner_update_lr=learn_inner_update_lr)\n",
        "\n",
        "    exp_string = f\"maml_sentiment_nway_{n_way}_kshot_{k_shot}_metabatch_{meta_batch_size}\"\n",
        "    meta_train_fn(model, exp_string, data_generator, n_way, meta_train_iterations, meta_batch_size, True, logdir, k_shot, num_inner_updates, meta_lr)\n",
        "\n"
      ],
      "metadata": {
        "id": "xqAj3CZhSXvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g2sYN4ldSXyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "za3x8qVSSX2p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}